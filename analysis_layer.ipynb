{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfd8274a-6a6e-4047-84ea-b17905756d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do\n",
    "#Split into multiple scripts\n",
    "# Apply robust enterprise level logging\n",
    "# Apply unit testing and integration testing logic.\n",
    "# Can I cache a particular table to speed up downstream processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6cee082-5deb-42f0-9d7c-b7f58b9bb8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'analysis_layer.ipynb', 'app.log', 'config.py', 'logging_config.py', 'output_file.parquet', 'raw_layer.py', 'stocks.csv', 'stock_analysis_2023.ipynb', 'stock_names_to_analyse_parquet', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "#install polyonc client\n",
    "#import relevant libraries or modules\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, explode, from_unixtime, regexp_replace, when, unix_timestamp, date_format\n",
    "from pyspark.sql.types import StructType, StructField, StringType,DoubleType,LongType, ArrayType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "from logging_config  import setup_logging\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import os\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ae3c167-7a9c-4747-a509-973840697771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      "\n",
      "+--------------------+------+\n",
      "|        company_name|symbol|\n",
      "+--------------------+------+\n",
      "|          Apple Inc.|  AAPL|\n",
      "|Microsoft Corpora...|  MSFT|\n",
      "|     Amazon.com Inc.|  AMZN|\n",
      "|           Tesla Inc|  TSLA|\n",
      "|Alphabet Inc. Cla...| GOOGL|\n",
      "|Alphabet Inc. Cla...|  GOOG|\n",
      "|Berkshire Hathawa...| BRK.B|\n",
      "|   Johnson & Johnson|   JNJ|\n",
      "|UnitedHealth Grou...|   UNH|\n",
      "|  NVIDIA Corporation|  NVDA|\n",
      "|Meta Platforms In...|  META|\n",
      "|Procter & Gamble ...|    PG|\n",
      "|JPMorgan Chase & Co.|   JPM|\n",
      "|Exxon Mobil Corpo...|   XOM|\n",
      "|   Visa Inc. Class A|     V|\n",
      "|     Home Depot Inc.|    HD|\n",
      "| Chevron Corporation|   CVX|\n",
      "|Mastercard Incorp...|    MA|\n",
      "|         AbbVie Inc.|  ABBV|\n",
      "|         Pfizer Inc.|   PFE|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of tickers: 100\n",
      "['AAPL', 'MSFT', 'AMZN', 'TSLA', 'GOOGL', 'GOOG', 'BRK.B', 'JNJ', 'UNH', 'NVDA', 'META', 'PG', 'JPM', 'XOM', 'V', 'HD', 'CVX', 'MA', 'ABBV', 'PFE', 'BAC', 'KO', 'COST', 'PEP', 'AVGO', 'LLY', 'WMT', 'CSCO', 'MRK', 'DIS', 'VZ', 'ABT', 'TMO', 'CMCSA', 'ACN', 'ADBE', 'MCD', 'INTC', 'WFC', 'CRM', 'BMY', 'DHR', 'PM', 'LIN', 'TXN', 'NKE', 'QCOM', 'UNP', 'RTX', 'NEE', 'MDT', 'AMGN', 'AMD', 'T', 'LOW', 'UPS', 'CVS', 'SPGI', 'HON', 'PLD', 'IBM', 'ELV', 'INTU', 'COP', 'ORCL', 'MS', 'AMT', 'CAT', 'TGT', 'AXP', 'LMT', 'DE', 'GS', 'SCHW', 'MO', 'C', 'PYPL', 'AMAT', 'ADP', 'BLK', 'BA', 'NOW', 'MDLZ', 'BKNG', 'GE', 'NFLX', 'ISRG', 'CB', 'SBUX', 'DUK', 'MMC', 'ZTS', 'SYK', 'MMM', 'CI', 'CCI', 'ADI', 'SO', 'GILD', 'CME']\n"
     ]
    }
   ],
   "source": [
    "#Create sparksession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"stock_analysis_2023_analysis_layer\"). \\\n",
    "        getOrCreate()\n",
    "\n",
    "def read_csv_into_df(stock_list):\n",
    "    #Read csv file\n",
    "    df_stock_name = spark.read.csv(stock_list, header=True, inferSchema=True)\n",
    "    \n",
    "    df_stock_name.write.parquet(\"stock_names_to_analyse_parquet\",mode=\"overwrite\")\n",
    "    df_read=spark.read.parquet(\"stock_names_to_analyse_parquet\")\n",
    "    df_read.printSchema()\n",
    "    df_stock_name = df_stock_name.withColumn('symbol', \n",
    "                    when(col(\"symbol\")==  'FB','META')\n",
    "                .when(col(\"symbol\")== 'ANTM','ELV')\n",
    "                                  .otherwise(col('symbol'))\n",
    "                                         )\n",
    "    \n",
    "    return df_stock_name\n",
    "\n",
    "def return_stock_names(df_stock_name):\n",
    "    #Get the list of ticker symbols from the dataframe.\n",
    "    symbols_to_analyse = [row['symbol'] for row in df_stock_name.collect()]\n",
    "    print(f\"Number of tickers: {len(symbols_to_analyse)}\")\n",
    "    return symbols_to_analyse\n",
    "    \n",
    "\n",
    "\n",
    "df_stock_name = read_csv_into_df(stock_list=\"stocks.csv\")\n",
    "df_stock_name.show()\n",
    "symbols_to_analyse=return_stock_names(df_stock_name)\n",
    "print(symbols_to_analyse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "572e8976-bcfb-44e7-8782-ff816f9a4c21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema before transformation:\n",
      "root\n",
      " |-- ticker: string (nullable = true)\n",
      " |-- c: double (nullable = true)\n",
      " |-- t: long (nullable = true)\n",
      "\n",
      "+------+\n",
      "|ticker|\n",
      "+------+\n",
      "|   MMM|\n",
      "|  CSCO|\n",
      "|  QCOM|\n",
      "|  META|\n",
      "|   DIS|\n",
      "|  TSLA|\n",
      "|     T|\n",
      "|    VZ|\n",
      "|   AMT|\n",
      "|   PFE|\n",
      "|  GOOG|\n",
      "|  NFLX|\n",
      "| GOOGL|\n",
      "|   CCI|\n",
      "|   WMT|\n",
      "|  AVGO|\n",
      "|  AMAT|\n",
      "|    GE|\n",
      "|  ORCL|\n",
      "|   LLY|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2024-01-02|\n",
      "|2024-01-03|\n",
      "|2024-01-04|\n",
      "|2024-01-05|\n",
      "|2024-01-08|\n",
      "|2024-01-09|\n",
      "|2024-01-10|\n",
      "|2024-01-11|\n",
      "|2024-01-12|\n",
      "|2024-01-16|\n",
      "|2024-01-17|\n",
      "|2024-01-18|\n",
      "|2024-01-19|\n",
      "|2024-01-22|\n",
      "|2024-01-23|\n",
      "|2024-01-24|\n",
      "|2024-01-25|\n",
      "|2024-01-26|\n",
      "|2024-01-29|\n",
      "|2024-01-30|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_data(input_file=\"output_file.parquet\"):\n",
    "    df=spark.read.parquet(input_file)\n",
    "    print(\"Schema before transformation:\")\n",
    "    df.printSchema()\n",
    "    if \"t\" in df.columns:\n",
    "        date_col = date_format(from_unixtime(col(\"t\")/1000),\"yyyy-MM-dd\")\n",
    "        df = df.withColumn(\"date\",date_col)\n",
    "        df=df.drop(col(\"t\"))\n",
    "    else:\n",
    "        print(\"column 't' is not found\")\n",
    "\n",
    "    df = df.withColumnRenamed(\"c\",\"close_price\")\n",
    "    return df\n",
    "df = read_data()\n",
    "df.select(col(\"ticker\")).distinct().show()\n",
    "df.select(col(\"date\")).distinct().orderBy(col(\"date\").asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8192f44-7755-40f8-afbc-700bddb4bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_price_difference( df, ticker_col='ticker', date_col='date',price_col='close_price'):\n",
    "    #1) Repartition by ticker means grouping/aggregations can be done locally (per ticker). This reduces shuffling and maximises parrallelism.\n",
    "    #data frame is partitioned and sorted for group operations\n",
    "    #2) Order by is needed due to make fetching the first and last prices are correct and efficient. \n",
    "    #3) Round, filtering, percent calcs completed in lower number of transformtions. Increases readability.\n",
    "    #4) Wrapping function allows reusuability, modularity, paramaterisation (for different datasets in other code bases), can write unit tests for small datasets\n",
    "    #and see if it has expected results.\n",
    "\n",
    "    df = df.repartition(ticker_col).orderBy(ticker_col,date_col)\n",
    "\n",
    "    window_spec = Window.partitionBy(\"ticker\").orderBy(date_col).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    \n",
    "    df_diff=df.withColumn(\"price_first\", F.first(price_col).over(window_spec)) \\\n",
    "                .withColumn(\"price_last\", F.last(price_col).over(window_spec)) \\\n",
    "                .withColumn(\"date_first\",F.first(date_col).over(window_spec)) \\\n",
    "                .withColumn(\"date_last\",F.last(date_col).over(window_spec)) \\\n",
    "                .filter((F.col(date_col) == F.col(\"date_first\")) | (F.col(date_col) == F.col(\"date_last\")))\n",
    "    \n",
    "    \n",
    "    df_diff=df_diff.withColumn(\"price_diff\", F.round(F.col(\"price_last\")-F.col(\"price_first\"),2)) \\\n",
    "                .withColumn(\"price_diff_percent\", F.round(col(\"price_diff\")*100/col(\"price_first\"),2))\n",
    "    \n",
    "    df_diff_percent=df_diff.select(\"ticker\", \"date_first\",\"date_first\", \"price_first\", \"date_last\",\"price_last\",\"price_diff\",\"price_diff_percent\"). \\\n",
    "                    distinct(). \\\n",
    "                    orderBy(F.col(\"price_diff_percent\").desc())\n",
    "    \n",
    "    return df_diff_percent\n",
    "\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17e4e894-4b19-4612-a31c-20e0cdf79951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+----------+----------+----------+------------------+\n",
      "|ticker|date_first|date_first|price_first|date_last |price_last|price_diff|price_diff_percent|\n",
      "+------+----------+----------+-----------+----------+----------+----------+------------------+\n",
      "|NVDA  |2023-02-24|2023-02-24|23.286     |2023-12-29|49.522    |26.24     |112.69            |\n",
      "|META  |2023-02-24|2023-02-24|170.39     |2023-12-29|353.96    |183.57    |107.74            |\n",
      "|INTC  |2023-02-24|2023-02-24|25.14      |2023-12-29|50.25     |25.11     |99.88             |\n",
      "|AVGO  |2023-02-24|2023-02-24|57.775     |2023-12-29|111.625   |53.85     |93.21             |\n",
      "|AMD   |2023-02-24|2023-02-24|78.09      |2023-12-29|147.41    |69.32     |88.77             |\n",
      "|ADBE  |2023-02-24|2023-02-24|320.54     |2023-12-29|596.6     |276.06    |86.12             |\n",
      "|LLY   |2023-02-24|2023-02-24|321.64     |2023-12-29|582.92    |261.28    |81.23             |\n",
      "|NOW   |2023-02-24|2023-02-24|425.59     |2023-12-29|706.49    |280.9     |66.0              |\n",
      "|AMZN  |2023-02-24|2023-02-24|93.5       |2023-12-29|151.94    |58.44     |62.5              |\n",
      "|CRM   |2023-02-24|2023-02-24|162.2      |2023-12-29|263.14    |100.94    |62.23             |\n",
      "|GOOG  |2023-02-24|2023-02-24|89.35      |2023-12-29|140.93    |51.58     |57.73             |\n",
      "|GOOGL |2023-02-24|2023-02-24|89.13      |2023-12-29|139.69    |50.56     |56.73             |\n",
      "|NFLX  |2023-02-24|2023-02-24|317.15     |2023-12-29|486.88    |169.73    |53.52             |\n",
      "|GE    |2023-02-24|2023-02-24|66.68      |2023-12-29|101.8595  |35.18     |52.76             |\n",
      "|MSFT  |2023-02-24|2023-02-24|249.22     |2023-12-29|376.04    |126.82    |50.89             |\n",
      "|INTU  |2023-02-24|2023-02-24|419.81     |2023-12-29|625.03    |205.22    |48.88             |\n",
      "|ISRG  |2023-02-24|2023-02-24|231.05     |2023-12-29|337.36    |106.31    |46.01             |\n",
      "|AMAT  |2023-02-24|2023-02-24|111.31     |2023-12-29|162.07    |50.76     |45.6              |\n",
      "|BKNG  |2023-02-24|2023-02-24|2452.48    |2023-12-29|3547.22   |1094.74   |44.64             |\n",
      "|COST  |2023-02-24|2023-02-24|488.61     |2023-12-29|660.08    |171.47    |35.09             |\n",
      "+------+----------+----------+-----------+----------+----------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------------+------+\n",
      "|company_name      |symbol|\n",
      "+------------------+------+\n",
      "|NVIDIA Corporation|NVDA  |\n",
      "+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "#QUESTION 1 ANSWER: Which stock has had the greatest relative increase in price in this period? 'NVDA'\n",
    "###############################\n",
    "\n",
    "def greatest_relative_increase(df_diff_percent):\n",
    "    #Get the first row and stock name.\n",
    "    stock_name = df_diff_percent.collect()[0][\"ticker\"]\n",
    "    stock_name =df_stock_name.filter(col(\"symbol\")==stock_name).show(truncate=False)\n",
    "    return stock_name\n",
    "    \n",
    "df_diff_percent= calculate_price_difference(df)\n",
    "df_diff_percent.show(truncate=False)\n",
    "greatest_relative_increase(df_diff_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564895fa-f19c-4be3-92d5-3b556b690e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26454937-a328-481c-8416-7d036f1cf7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b0a860c-72b0-4c80-b452-3f5a76425a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1173908.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find how much this percentage increase would increase your intial portfolio\n",
    "#At the end of the year.\n",
    "def final_portfolio_value(df_diff_percent):\n",
    "    #Find the average percentage increase over the group of stocks in 2023.\n",
    "    df_diff_percent_average=df_diff_percent.agg(F.avg(\"price_diff_percent\").alias(\"avg_percent_gain\"))\n",
    "    df_diff_percent_average=df_diff_percent_average.collect()[0][\"avg_percent_gain\"]\n",
    "    \n",
    "    initial_portfolio_value=1000000\n",
    "    final_portfolio_value=initial_portfolio_value*(1+df_diff_percent_average/100)\n",
    "    final_portfolio_value = round(final_portfolio_value,2)\n",
    "    return final_portfolio_value\n",
    "#QUESTION 2 ANSWER: If you had invested $1 million at the beginning of this period by purchasing $10,000 worth of shares in every company in the list equally, \n",
    "#how much would you have today? Technical note, you can assume that it is possible \n",
    "#to purchase fractional shares. Ans: $1682181.14\n",
    "final_portfolio_value(df_diff_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca7d3fee-b655-4a71-9d56-9c6e4261225c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema before transformation:\n",
      "root\n",
      " |-- ticker: string (nullable = true)\n",
      " |-- c: double (nullable = true)\n",
      " |-- t: long (nullable = true)\n",
      "\n",
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2023-02-24|\n",
      "|2023-02-27|\n",
      "|2023-02-28|\n",
      "|2023-03-01|\n",
      "|2023-03-02|\n",
      "|2023-03-03|\n",
      "|2023-03-06|\n",
      "|2023-03-07|\n",
      "|2023-03-08|\n",
      "|2023-03-09|\n",
      "|2023-03-10|\n",
      "|2023-03-13|\n",
      "|2023-03-14|\n",
      "|2023-03-15|\n",
      "|2023-03-16|\n",
      "|2023-03-17|\n",
      "|2023-03-20|\n",
      "|2023-03-21|\n",
      "|2023-03-22|\n",
      "|2023-03-23|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Start Question 3\n",
    "df = read_data()\n",
    "df.select(col(\"date\")).distinct().orderBy(col(\"date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e27236c1-12e1-4c32-8d32-9e5f2fb3aed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema before transformation:\n",
      "root\n",
      " |-- ticker: string (nullable = true)\n",
      " |-- c: double (nullable = true)\n",
      " |-- t: long (nullable = true)\n",
      "\n",
      "+------+-----------+----------+\n",
      "|ticker|close_price|      date|\n",
      "+------+-----------+----------+\n",
      "|    GE|    85.1875|2023-06-13|\n",
      "|    GE|    83.5595|2023-06-14|\n",
      "|    GE|    84.1261|2023-06-15|\n",
      "|    GE|    84.8284|2023-06-16|\n",
      "|    GE|    83.0168|2023-06-20|\n",
      "|    GE|    83.8388|2023-06-21|\n",
      "|    GE|    83.6552|2023-06-22|\n",
      "|    GE|    82.8252|2023-06-23|\n",
      "|    GE|    83.3599|2023-06-26|\n",
      "|    GE|     83.735|2023-06-27|\n",
      "|    GE|     85.435|2023-06-28|\n",
      "|    GE|    85.9856|2023-06-29|\n",
      "|    GE|    87.6696|2023-06-30|\n",
      "|    GE|    86.4166|2023-07-03|\n",
      "|    GE|    86.7119|2023-07-05|\n",
      "|    GE|    85.6903|2023-07-06|\n",
      "|    GE|    86.4086|2023-07-07|\n",
      "|    GE|    88.2123|2023-07-10|\n",
      "|    GE|     88.763|2023-07-11|\n",
      "|    GE|     88.747|2023-07-12|\n",
      "+------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+-----------+----+\n",
      "|ticker|close_price|date|\n",
      "+------+-----------+----+\n",
      "+------+-----------+----+\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "df_start dataset is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m df \u001b[38;5;241m=\u001b[39m read_data()\n\u001b[0;32m     46\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 47\u001b[0m stock_with_greatest_monthly_CAGR\u001b[38;5;241m=\u001b[39m\u001b[43mmonthly_cagr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_month\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2023-01-03\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_month\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2023-06-30\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m, in \u001b[0;36mmonthly_cagr\u001b[1;34m(df, start_month, end_month)\u001b[0m\n\u001b[0;32m      7\u001b[0m df_start\u001b[38;5;241m=\u001b[39mdf_start\u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_month\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df_start\u001b[38;5;241m.\u001b[39misEmpty():\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_start dataset is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#Shows the end date and end prince we are interested in. \u001b[39;00m\n\u001b[0;32m     12\u001b[0m df_end\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m==\u001b[39mend_month)\n",
      "\u001b[1;31mValueError\u001b[0m: df_start dataset is empty"
     ]
    }
   ],
   "source": [
    "def monthly_cagr(df, start_month, end_month):\n",
    "\n",
    "    #Filter dataframe for data on these dates. Make 2 seperate dataframes for each of these dates. \n",
    "    df_start=df.filter(col(\"date\")==start_month)\n",
    "    df_start.show()\n",
    "    df_start=df_start.withColumnRenamed(\"close_price\",\"start_price\")\n",
    "    df_start=df_start.withColumnRenamed(\"date\",\"start_month\")\n",
    "    if df_start.isEmpty():\n",
    "        raise ValueError(\"df_start dataset is empty\")\n",
    "\n",
    "    #Shows the end date and end prince we are interested in. \n",
    "    df_end=df.filter(col(\"date\")==end_month)\n",
    "    df_end=df_end.withColumnRenamed(\"close_price\",\"end_price\")\n",
    "    df_end=df_end.withColumnRenamed(\"date\",\"end_month\")\n",
    "    if df_end.isEmpty():\n",
    "        raise ValueError(\"df_end dataset is empty\")\n",
    "\n",
    "    #Join these dataframes on primary key ticker. \n",
    "    df_jan_jun=df_start.join(df_end,on=\"ticker\")\n",
    "\n",
    "    #For extensibilty for the month constant in below calc\n",
    "    start_date=datetime.strptime(start_month,\"%Y-%m-%d\")\n",
    "    end_date=datetime.strptime(end_month,\"%Y-%m-%d\")\n",
    "    difference=relativedelta(end_date,start_date)\n",
    "    difference_months=difference.years*12+difference.months\n",
    "\n",
    "    #calculate the monthly Compounded annual growth rate.\n",
    "    #Assumption of inclusive of January and June months\n",
    "    end_val = col(\"end_price\")\n",
    "    start_val = col(\"start_price\")\n",
    "    months = difference_months+1\n",
    "    \n",
    "    calculation = ((end_val/start_val)**(1/months))-1\n",
    "    df_jan_jun=df_jan_jun.withColumn(\"CAGR over defined period\", calculation)\n",
    "\n",
    "    #Sort datafram in \"percent_gain\" decending order\n",
    "    df_jan_jun=df_jan_jun.sort(col(\"CAGR over defined period\"), ascending=False)\n",
    "\n",
    "    #Pick the top result\n",
    "    stock_with_greatest_monthly_CAGR=df_jan_jun.collect()[0][\"ticker\"]\n",
    "    stock_with_greatest_monthly_CAGR=df_stock_name.filter(col(\"symbol\")==stock_with_greatest_monthly_CAGR)\n",
    "    stock_with_greatest_monthly_CAGR.show(truncate=False)\n",
    "    return print(stock_with_greatest_monthly_CAGR.collect()[0][\"company_name\"])\n",
    "    \n",
    "df = read_data()\n",
    "df.show()\n",
    "stock_with_greatest_monthly_CAGR=monthly_cagr(df, start_month=\"2023-01-03\", end_month=\"2023-06-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961244a8-2dbc-4f2a-b901-7654e73d5c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a91cb-285d-4f44-b9d9-1eb00183d983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248eb9e-b833-4ca9-8428-f57e6b2f4e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7482d3d-cf89-42c6-996a-64106dc78699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Question 4\n",
    "def greatest_decrease_in_price(df):\n",
    "\n",
    "    df_week = df.withColumn(\"week_start\",F.date_trunc(\"week\",col(\"date\")))\n",
    "    #Window partition by ticker and week\n",
    "    window_spec=Window.partitionBy(\"ticker\", \"week_start\").orderBy(\"week_start\")\n",
    "    df_week=df_week.withColumn(\"start_price\",F.first(\"close_price\").over(window_spec)) \\\n",
    "                .withColumn(\"end_price\",F.last(\"close_price\").over(window_spec))\n",
    "    df_week_final=df_week.select(\"ticker\",\"week_start\",\"start_price\",\"end_price\").distinct()\n",
    "    df_week_final=df_week_final.withColumn(\"price_diff\",F.round(F.col(\"end_price\")-F.col(\"start_price\"),2))\n",
    "    df_week_final = df_week_final.sort(col(\"price_diff\"),ascending=True)\n",
    "    df_week_final.show()\n",
    "    greatest_price_drop_stock = df_week_final.collect()[0][\"ticker\"]\n",
    "    greatest_price_drop_week = df_week_final.collect()[0][\"week_start\"]\n",
    "    print(greatest_price_drop_stock)\n",
    "    print(greatest_price_drop_week)\n",
    "    greatest_price_drop_stock_name= df_stock_name.filter(col(\"symbol\")==greatest_price_drop_stock)\n",
    "    greatest_price_drop_stock_name.show()\n",
    "    return greatest_price_drop_stock_name.collect()[0][\"company_name\"]\n",
    "greatest_decrease_in_price(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e0b42-1918-416a-b247-9e3334d7a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greatest_percent_drop_stock(df):\n",
    "    df_week = df.withColumn(\"week_start\",F.date_trunc(\"week\",col(\"date\")))\n",
    "    #Window partition by ticker and week\n",
    "    window_spec=Window.partitionBy(\"ticker\", \"week_start\").orderBy(\"week_start\")\n",
    "    df_week=df_week.withColumn(\"start_price\",F.first(\"close_price\").over(window_spec)) \\\n",
    "                  .withColumn(\"end_price\",F.last(\"close_price\").over(window_spec))\n",
    "    df_week_final=df_week.select(\"ticker\",\"week_start\",\"start_price\",\"end_price\").distinct()\n",
    "    df_week_final=df_week_final.withColumn(\"price_diff\",F.round(F.col(\"end_price\")-F.col(\"start_price\"),2))\n",
    "    calc= (col(\"price_diff\"))*100/col(\"start_price\")\n",
    "    df_week_final_percent=df_week_final.withColumn(\"percent_drop\",calc). \\\n",
    "                            sort(col(\"percent_drop\"),ascending=True)\n",
    "    \n",
    "    \n",
    "    greatest_percent_drop_stock = df_week_final_percent.collect()[0][\"ticker\"]    \n",
    "    greatest_percent_drop_week =df_week_final_percent.collect()[0][\"week_start\"]\n",
    "    \n",
    "    print(greatest_percent_drop_week)\n",
    "    \n",
    "    greatest_percent_drop_stock_full_name = df_stock_name.filter(col(\"symbol\")==greatest_percent_drop_stock)\n",
    "    greatest_percent_drop_stock_full_name = greatest_percent_drop_stock_full_name.collect()[0][\"company_name\"]\n",
    "    print(greatest_percent_drop_stock_full_name)\n",
    "    return\n",
    "    \n",
    "greatest_percent_drop_stock(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd4bfd-083a-4a56-9853-2777c2fbea7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2b6f8-3a33-4a26-bef8-0afefa94bdf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb7d22-2254-404f-85b5-cac3ee21c75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
