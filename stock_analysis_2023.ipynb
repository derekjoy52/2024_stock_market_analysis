{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70e14ca-1c23-468f-8aab-a7ae9f664afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do\n",
    "#Split into multiple scripts\n",
    "# Apply robust enterprise level logging\n",
    "# Apply unit testing and integration testing logic.\n",
    "# Can I cache a particular table to speed up downstream processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "3bbc192e-ab52-4b2e-a4f5-b5d4d420b3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polygon-api-client in c:\\users\\derek\\anaconda3\\lib\\site-packages (1.14.2)\n",
      "Requirement already satisfied: certifi<2025.0.0,>=2022.5.18 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from polygon-api-client) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.9 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from polygon-api-client) (1.26.18)\n",
      "Requirement already satisfied: websockets<13.0,>=10.3 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from polygon-api-client) (12.0)\n"
     ]
    }
   ],
   "source": [
    "#install polyonc client\n",
    "!pip install polygon-api-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17adba90-acce-45fa-a6ac-6ccdc8a20082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.5.0\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: c:\\users\\derek\\anaconda3\\lib\\site-packages\n",
      "Requires: py4j\n",
      "Required-by: delta-spark\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6cee082-5deb-42f0-9d7c-b7f58b9bb8a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'logging_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogging_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_logging\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'logging_config'"
     ]
    }
   ],
   "source": [
    "#import relevant libraries or modules\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, explode, from_unixtime, regexp_replace, when, unix_timestamp, date_format\n",
    "from pyspark.sql.types import StructType, StructField, StringType,DoubleType,LongType, ArrayType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "import logging\n",
    "from logging_config import setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ae3c167-7a9c-4747-a509-973840697771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      "\n",
      "Number of tickers: 100\n"
     ]
    }
   ],
   "source": [
    "#Create sparksession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"stock_analysis_2023\"). \\\n",
    "        getOrCreate()\n",
    "\n",
    "def read_csv_into_df(stock_list):\n",
    "    #Read csv file\n",
    "    df_stock_name = spark.read.csv(stock_list, header=True, inferSchema=True)\n",
    "    \n",
    "    df_stock_name.write.parquet(\"stock_names_to_analyse_parquet\",mode=\"overwrite\")\n",
    "    df_read=spark.read.parquet(\"stock_names_to_analyse_parquet\")\n",
    "    df_read.printSchema()\n",
    "    df = df_stock_name.withColumn('symbol', \n",
    "                    when(col(\"symbol\")==  'FB','META')\n",
    "                .when(col(\"symbol\")== 'ANTM','ELV')\n",
    "                                  .otherwise(col('symbol'))\n",
    "                                         )\n",
    "    \n",
    "    #Get the list of ticker symbols from the dataframe.\n",
    "    symbols_to_analyse = [row['symbol'] for row in df.select(\"symbol\").collect()]\n",
    "    \n",
    "    #check we have 100\n",
    "    print(f\"Number of tickers: {len(symbols_to_analyse)}\")\n",
    "    return symbols_to_analyse\n",
    "\n",
    "symbols_to_analyse=read_csv_into_df(stock_list=\"stocks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bc64642-6d69-4be2-8c50-530f6c95fe5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAPL',\n",
       " 'MSFT',\n",
       " 'AMZN',\n",
       " 'TSLA',\n",
       " 'GOOGL',\n",
       " 'GOOG',\n",
       " 'BRK.B',\n",
       " 'JNJ',\n",
       " 'UNH',\n",
       " 'NVDA',\n",
       " 'META',\n",
       " 'PG',\n",
       " 'JPM',\n",
       " 'XOM',\n",
       " 'V',\n",
       " 'HD',\n",
       " 'CVX',\n",
       " 'MA',\n",
       " 'ABBV',\n",
       " 'PFE',\n",
       " 'BAC',\n",
       " 'KO',\n",
       " 'COST',\n",
       " 'PEP',\n",
       " 'AVGO',\n",
       " 'LLY',\n",
       " 'WMT',\n",
       " 'CSCO',\n",
       " 'MRK',\n",
       " 'DIS',\n",
       " 'VZ',\n",
       " 'ABT',\n",
       " 'TMO',\n",
       " 'CMCSA',\n",
       " 'ACN',\n",
       " 'ADBE',\n",
       " 'MCD',\n",
       " 'INTC',\n",
       " 'WFC',\n",
       " 'CRM',\n",
       " 'BMY',\n",
       " 'DHR',\n",
       " 'PM',\n",
       " 'LIN',\n",
       " 'TXN',\n",
       " 'NKE',\n",
       " 'QCOM',\n",
       " 'UNP',\n",
       " 'RTX',\n",
       " 'NEE',\n",
       " 'MDT',\n",
       " 'AMGN',\n",
       " 'AMD',\n",
       " 'T',\n",
       " 'LOW',\n",
       " 'UPS',\n",
       " 'CVS',\n",
       " 'SPGI',\n",
       " 'HON',\n",
       " 'PLD',\n",
       " 'IBM',\n",
       " 'ELV',\n",
       " 'INTU',\n",
       " 'COP',\n",
       " 'ORCL',\n",
       " 'MS',\n",
       " 'AMT',\n",
       " 'CAT',\n",
       " 'TGT',\n",
       " 'AXP',\n",
       " 'LMT',\n",
       " 'DE',\n",
       " 'GS',\n",
       " 'SCHW',\n",
       " 'MO',\n",
       " 'C',\n",
       " 'PYPL',\n",
       " 'AMAT',\n",
       " 'ADP',\n",
       " 'BLK',\n",
       " 'BA',\n",
       " 'NOW',\n",
       " 'MDLZ',\n",
       " 'BKNG',\n",
       " 'GE',\n",
       " 'NFLX',\n",
       " 'ISRG',\n",
       " 'CB',\n",
       " 'SBUX',\n",
       " 'DUK',\n",
       " 'MMC',\n",
       " 'ZTS',\n",
       " 'SYK',\n",
       " 'MMM',\n",
       " 'CI',\n",
       " 'CCI',\n",
       " 'ADI',\n",
       " 'SO',\n",
       " 'GILD',\n",
       " 'CME']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_to_analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab1f6301-62f1-4cf7-a8d7-d6d16ad97e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for AAPL...\n",
      "Fetching data for MSFT...\n",
      "Fetching data for AMZN...\n",
      "Fetching data for TSLA...\n",
      "Fetching data for GOOGL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:06:09,193 - INFO - Sucessfully processed ticker: AAPL with 250 records.\n",
      "2024-12-04 23:06:09,193 - INFO - Sucessfully processed ticker: MSFT with 250 records.\n",
      "2024-12-04 23:06:09,193 - INFO - Sucessfully processed ticker: AMZN with 250 records.\n",
      "2024-12-04 23:06:09,193 - INFO - Sucessfully processed ticker: TSLA with 250 records.\n",
      "2024-12-04 23:06:18,756 - INFO - Sucessfully processed ticker: GOOGL with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written with overwrite\n",
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for GOOG...\n",
      "Fetching data for BRK.B...\n",
      "Fetching data for JNJ...\n",
      "Fetching data for UNH...\n",
      "Fetching data for NVDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:07:28,179 - INFO - Sucessfully processed ticker: GOOG with 250 records.\n",
      "2024-12-04 23:07:28,195 - INFO - Sucessfully processed ticker: BRK.B with 250 records.\n",
      "2024-12-04 23:07:28,197 - INFO - Sucessfully processed ticker: JNJ with 250 records.\n",
      "2024-12-04 23:07:36,372 - INFO - Sucessfully processed ticker: UNH with 250 records.\n",
      "2024-12-04 23:07:36,372 - INFO - Sucessfully processed ticker: NVDA with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written with append\n",
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for META...\n",
      "Fetching data for PG...\n",
      "Fetching data for JPM...\n",
      "Fetching data for XOM...\n",
      "Fetching data for V...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:08:47,943 - INFO - Sucessfully processed ticker: META with 250 records.\n",
      "2024-12-04 23:08:47,943 - INFO - Sucessfully processed ticker: PG with 250 records.\n",
      "2024-12-04 23:08:55,557 - INFO - Sucessfully processed ticker: JPM with 250 records.\n",
      "2024-12-04 23:08:55,557 - INFO - Sucessfully processed ticker: XOM with 250 records.\n",
      "2024-12-04 23:08:55,565 - INFO - Sucessfully processed ticker: V with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written with append\n",
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for HD...\n",
      "Fetching data for CVX...\n",
      "Fetching data for MA...\n",
      "Fetching data for ABBV...\n",
      "Fetching data for PFE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:10:06,165 - INFO - Sucessfully processed ticker: HD with 250 records.\n",
      "2024-12-04 23:10:13,386 - INFO - Sucessfully processed ticker: CVX with 250 records.\n",
      "2024-12-04 23:10:13,386 - INFO - Sucessfully processed ticker: MA with 250 records.\n",
      "2024-12-04 23:10:13,386 - INFO - Sucessfully processed ticker: ABBV with 250 records.\n",
      "2024-12-04 23:10:13,401 - INFO - Sucessfully processed ticker: PFE with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for BAC...\n",
      "Fetching data for KO...\n",
      "Fetching data for COST...\n",
      "Fetching data for PEP...\n",
      "Fetching data for AVGO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:11:21,728 - INFO - Sucessfully processed ticker: BAC with 250 records.\n",
      "2024-12-04 23:11:21,728 - INFO - Sucessfully processed ticker: KO with 250 records.\n",
      "2024-12-04 23:11:21,728 - INFO - Sucessfully processed ticker: COST with 250 records.\n",
      "2024-12-04 23:11:21,743 - INFO - Sucessfully processed ticker: PEP with 250 records.\n",
      "2024-12-04 23:11:30,933 - INFO - Sucessfully processed ticker: AVGO with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written with append\n",
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for LLY...\n",
      "Fetching data for WMT...\n",
      "Fetching data for CSCO...\n",
      "Fetching data for MRK...\n",
      "Fetching data for DIS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:12:42,744 - INFO - Sucessfully processed ticker: LLY with 250 records.\n",
      "2024-12-04 23:12:42,746 - INFO - Sucessfully processed ticker: WMT with 250 records.\n",
      "2024-12-04 23:12:42,749 - INFO - Sucessfully processed ticker: CSCO with 250 records.\n",
      "2024-12-04 23:12:50,934 - INFO - Sucessfully processed ticker: MRK with 250 records.\n",
      "2024-12-04 23:12:50,934 - INFO - Sucessfully processed ticker: DIS with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written with append\n",
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for VZ...\n",
      "Fetching data for ABT...\n",
      "Fetching data for TMO...\n",
      "Fetching data for CMCSA...\n",
      "Fetching data for ACN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:14:01,079 - INFO - Sucessfully processed ticker: VZ with 250 records.\n",
      "2024-12-04 23:14:01,087 - INFO - Sucessfully processed ticker: ABT with 250 records.\n",
      "2024-12-04 23:14:10,119 - INFO - Sucessfully processed ticker: TMO with 250 records.\n",
      "2024-12-04 23:14:10,119 - INFO - Sucessfully processed ticker: CMCSA with 250 records.\n",
      "2024-12-04 23:14:10,130 - INFO - Sucessfully processed ticker: ACN with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written with append\n",
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for ADBE...\n",
      "Fetching data for MCD...\n",
      "Fetching data for INTC...\n",
      "Fetching data for WFC...\n",
      "Fetching data for CRM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:15:22,065 - INFO - Sucessfully processed ticker: ADBE with 250 records.\n",
      "2024-12-04 23:15:29,904 - INFO - Sucessfully processed ticker: MCD with 250 records.\n",
      "2024-12-04 23:15:29,904 - INFO - Sucessfully processed ticker: INTC with 250 records.\n",
      "2024-12-04 23:15:29,920 - INFO - Sucessfully processed ticker: WFC with 250 records.\n",
      "2024-12-04 23:15:29,920 - INFO - Sucessfully processed ticker: CRM with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for BMY...\n",
      "Fetching data for DHR...\n",
      "Fetching data for PM...\n",
      "Fetching data for LIN...\n",
      "Fetching data for TXN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:16:41,529 - INFO - Sucessfully processed ticker: BMY with 250 records.\n",
      "2024-12-04 23:16:41,545 - INFO - Sucessfully processed ticker: DHR with 250 records.\n",
      "2024-12-04 23:16:41,545 - INFO - Sucessfully processed ticker: PM with 250 records.\n",
      "2024-12-04 23:16:41,551 - INFO - Sucessfully processed ticker: LIN with 250 records.\n",
      "2024-12-04 23:16:49,246 - INFO - Sucessfully processed ticker: TXN with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written with append\n",
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for NKE...\n",
      "Fetching data for QCOM...\n",
      "Fetching data for UNP...\n",
      "Fetching data for RTX...\n",
      "Fetching data for NEE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:17:59,462 - INFO - Sucessfully processed ticker: NKE with 250 records.\n",
      "2024-12-04 23:17:59,462 - INFO - Sucessfully processed ticker: QCOM with 250 records.\n",
      "2024-12-04 23:17:59,478 - INFO - Sucessfully processed ticker: UNP with 250 records.\n",
      "2024-12-04 23:18:08,259 - INFO - Sucessfully processed ticker: RTX with 250 records.\n",
      "2024-12-04 23:18:08,259 - INFO - Sucessfully processed ticker: NEE with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written with append\n",
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for MDT...\n",
      "Fetching data for AMGN...\n",
      "Fetching data for AMD...\n",
      "Fetching data for T...\n",
      "Fetching data for LOW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:19:19,438 - INFO - Sucessfully processed ticker: MDT with 250 records.\n",
      "2024-12-04 23:19:19,438 - INFO - Sucessfully processed ticker: AMGN with 250 records.\n",
      "2024-12-04 23:19:27,075 - INFO - Sucessfully processed ticker: AMD with 250 records.\n",
      "2024-12-04 23:19:27,075 - INFO - Sucessfully processed ticker: T with 250 records.\n",
      "2024-12-04 23:19:27,075 - INFO - Sucessfully processed ticker: LOW with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written with append\n",
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:20:34,520 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /v2/aggs/ticker/UPS/range/1/day/2023-01-01/2023-12-31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for UPS...\n",
      "Fetching data for CVS...\n",
      "Fetching data for SPGI...\n",
      "Fetching data for HON...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:20:36,588 - INFO - Sucessfully processed ticker: UPS with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for PLD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:20:46,122 - INFO - Sucessfully processed ticker: CVS with 250 records.\n",
      "2024-12-04 23:20:46,122 - INFO - Sucessfully processed ticker: SPGI with 250 records.\n",
      "2024-12-04 23:20:46,122 - INFO - Sucessfully processed ticker: HON with 250 records.\n",
      "2024-12-04 23:20:46,122 - INFO - Sucessfully processed ticker: PLD with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for IBM...\n",
      "Fetching data for ELV...\n",
      "Fetching data for INTU...\n",
      "Fetching data for COP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:21:56,330 - INFO - Sucessfully processed ticker: IBM with 250 records.\n",
      "2024-12-04 23:21:56,330 - INFO - Sucessfully processed ticker: ELV with 250 records.\n",
      "2024-12-04 23:21:56,330 - INFO - Sucessfully processed ticker: INTU with 250 records.\n",
      "2024-12-04 23:21:56,330 - INFO - Sucessfully processed ticker: COP with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for ORCL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:22:07,700 - INFO - Sucessfully processed ticker: ORCL with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written with append\n",
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for MS...\n",
      "Fetching data for AMT...\n",
      "Fetching data for CAT...\n",
      "Fetching data for TGT...\n",
      "Fetching data for AXP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:23:19,287 - INFO - Sucessfully processed ticker: MS with 250 records.\n",
      "2024-12-04 23:23:19,294 - INFO - Sucessfully processed ticker: AMT with 250 records.\n",
      "2024-12-04 23:23:19,294 - INFO - Sucessfully processed ticker: CAT with 250 records.\n",
      "2024-12-04 23:23:27,819 - INFO - Sucessfully processed ticker: TGT with 250 records.\n",
      "2024-12-04 23:23:27,819 - INFO - Sucessfully processed ticker: AXP with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written with append\n",
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for LMT...\n",
      "Fetching data for DE...\n",
      "Fetching data for GS...\n",
      "Fetching data for SCHW...\n",
      "Fetching data for MO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:24:39,364 - INFO - Sucessfully processed ticker: LMT with 250 records.\n",
      "2024-12-04 23:24:39,364 - INFO - Sucessfully processed ticker: DE with 250 records.\n",
      "2024-12-04 23:24:47,081 - INFO - Sucessfully processed ticker: GS with 250 records.\n",
      "2024-12-04 23:24:47,081 - INFO - Sucessfully processed ticker: SCHW with 250 records.\n",
      "2024-12-04 23:24:47,081 - INFO - Sucessfully processed ticker: MO with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written with append\n",
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for C...\n",
      "Fetching data for PYPL...\n",
      "Fetching data for AMAT...\n",
      "Fetching data for ADP...\n",
      "Fetching data for BLK...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:25:57,696 - INFO - Sucessfully processed ticker: C with 250 records.\n",
      "2024-12-04 23:26:06,166 - INFO - Sucessfully processed ticker: PYPL with 250 records.\n",
      "2024-12-04 23:26:06,166 - INFO - Sucessfully processed ticker: AMAT with 250 records.\n",
      "2024-12-04 23:26:06,166 - INFO - Sucessfully processed ticker: ADP with 250 records.\n",
      "2024-12-04 23:26:06,166 - INFO - Sucessfully processed ticker: BLK with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for BA...\n",
      "Fetching data for NOW...\n",
      "Fetching data for MDLZ...\n",
      "Fetching data for BKNG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:27:18,088 - INFO - Sucessfully processed ticker: BA with 250 records.\n",
      "2024-12-04 23:27:18,088 - INFO - Sucessfully processed ticker: NOW with 250 records.\n",
      "2024-12-04 23:27:18,088 - INFO - Sucessfully processed ticker: MDLZ with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for GE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:27:18,103 - INFO - Sucessfully processed ticker: BKNG with 250 records.\n",
      "2024-12-04 23:27:26,553 - INFO - Sucessfully processed ticker: GE with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written with append\n",
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for NFLX...\n",
      "Fetching data for ISRG...\n",
      "Fetching data for CB...\n",
      "Fetching data for SBUX...\n",
      "Fetching data for DUK...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:28:38,364 - INFO - Sucessfully processed ticker: NFLX with 250 records.\n",
      "2024-12-04 23:28:38,364 - INFO - Sucessfully processed ticker: ISRG with 250 records.\n",
      "2024-12-04 23:28:38,364 - INFO - Sucessfully processed ticker: CB with 250 records.\n",
      "2024-12-04 23:28:46,411 - INFO - Sucessfully processed ticker: SBUX with 250 records.\n",
      "2024-12-04 23:28:46,411 - INFO - Sucessfully processed ticker: DUK with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written with append\n",
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for MMC...\n",
      "Fetching data for ZTS...\n",
      "Fetching data for SYK...\n",
      "Fetching data for MMM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:29:56,797 - INFO - Sucessfully processed ticker: MMC with 250 records.\n",
      "2024-12-04 23:29:56,813 - INFO - Sucessfully processed ticker: ZTS with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for CI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:30:04,709 - INFO - Sucessfully processed ticker: SYK with 250 records.\n",
      "2024-12-04 23:30:04,709 - INFO - Sucessfully processed ticker: MMM with 250 records.\n",
      "2024-12-04 23:30:04,709 - INFO - Sucessfully processed ticker: CI with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written with append\n",
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n",
      "Fetching data for CCI...\n",
      "Fetching data for ADI...\n",
      "Fetching data for SO...\n",
      "Fetching data for GILD...\n",
      "Fetching data for CME...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 23:31:18,521 - INFO - Sucessfully processed ticker: CCI with 250 records.\n",
      "2024-12-04 23:31:28,312 - INFO - Sucessfully processed ticker: ADI with 250 records.\n",
      "2024-12-04 23:31:28,326 - INFO - Sucessfully processed ticker: SO with 250 records.\n",
      "2024-12-04 23:31:28,326 - INFO - Sucessfully processed ticker: GILD with 250 records.\n",
      "2024-12-04 23:31:28,326 - INFO - Sucessfully processed ticker: CME with 250 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing of a batch complete\n",
      "Waiting 60.1secs\n"
     ]
    }
   ],
   "source": [
    "from polygon import RESTClient\n",
    "import time #Need to add delays between API request as there is a limit of 5calls/minute\n",
    "import config\n",
    "import json\n",
    "from typing import cast\n",
    "from urllib3 import HTTPResponse\n",
    "\n",
    "#Set the constants we need for the API\n",
    "time_frame='day'\n",
    "start_date= '2023-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "#Initialise the API client\n",
    "client = RESTClient(config.API_KEY)\n",
    "batch_size=5\n",
    "\n",
    "#Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, #Set the logging level\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\", #Define log format\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"app.log\"), #Log to a file\n",
    "        logging.StreamHandler() #Log to the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def fetch_batch(batch):\n",
    "\n",
    "    try:\n",
    "        data=[]\n",
    "        #Requests data for a ticker one by one.\n",
    "        for ticker in batch:\n",
    "            print(f\"Fetching data for {ticker}...\")\n",
    "            aggregate = client.get_aggs(\n",
    "                    ticker,\n",
    "                    1,  #Aggregation multiplier\n",
    "                    time_frame, \n",
    "                    start_date,\n",
    "                    end_date,\n",
    "                    raw = True #Requests raw JSON response\n",
    "                    )\n",
    "            #Pase the JSON response\n",
    "            response_data = json.loads(aggregate.data)\n",
    "\n",
    "            \n",
    "            if response_data:\n",
    "                #appends the relevant data we need to a list\n",
    "                data.append({\n",
    "                    'ticker':ticker,\n",
    "                    'results':response_data['results']\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"No results found for {ticker}\")\n",
    "        return data\n",
    "    #Error handling\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching data for {batch}: {e}\")\n",
    "        return []\n",
    "\n",
    "#Retries API call incase of failure due to network issues. \n",
    "def fetch_batch_with_rety(batch,retries=2,delay=60.1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return fetch_batch(batch)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching data for batch {batch}:{e}\")\n",
    "            if attempt<retries-1:\n",
    "                logger.error(f\"Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logger.error(f\"Failed after {retries} attempts.\")\n",
    "                return []\n",
    "def flat_map(entry):\n",
    "    try:\n",
    "        #ensure ticker exists and is not a string\n",
    "        ticker = entry['ticker']\n",
    "        if not isinstance(ticker,str):\n",
    "            raise TypeError(f\"Expected 'ticker' to be string, got {type(ticker)}\")\n",
    "        #Process the data field\n",
    "        if 'data' not in entry or not isinstance(entry['data'],list):\n",
    "            raise ValueError(f\"Expected 'data' to be a list but got {type(entry.get('data'))}\")\n",
    "        \n",
    "        result = []\n",
    "        for item in entry['data']:\n",
    "            c = float(item.get(\"c\",0))\n",
    "            t = item.get('t',0)\n",
    "\n",
    "\n",
    "            if not isinstance(t,int) or t<0:\n",
    "                raise ValueError(f\"Invalid timestamp value for ticker {ticker}:{t}. Expected a non-negative integer.\")\n",
    "\n",
    "            result.append((ticker,c,t))\n",
    "        logging.info(f\"Sucessfully processed ticker: {ticker} with {len(entry['data'])} records.\")\n",
    "        return result\n",
    "\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"Missing key in entry: {entry}, KeyError: {e}\")\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Value Error in entry: {entry}, ValueError: {e}\")\n",
    "    except TypeError as e:\n",
    "        logging.error(f\"Type Error in entry: {entry}, TypeError: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Unexpected Error in processing: {entry}, Error as {e}\", exc_info=True)\n",
    "    #Return empty list if there is an error    \n",
    "    return []\n",
    "\n",
    "\n",
    "def process_all_tickers_and_write(symbols_to_analyse):\n",
    "    #Processes all tickers by fetching it in batches\n",
    "    #I'll ge the list to hold a 1000records then I do a write to disk and cleam memory.\n",
    "    write_batch_size=1000\n",
    "    accumulated_result = []\n",
    "    for i in range(0,len(symbols_to_analyse), batch_size):\n",
    "        #Iterate through tickers in batches of 5.\n",
    "        batch=symbols_to_analyse[i:i+batch_size]\n",
    "        #Call above function on each batch\n",
    "        batch_data = fetch_batch(batch)\n",
    "\n",
    "        #Ticker data should in a dictionary format\n",
    "        for ticker_data in batch_data:\n",
    "            if not isinstance(ticker_data,dict):\n",
    "                print(f\"Unexpected data format: {ticker_data}\")\n",
    "                continue\n",
    "            \n",
    "            #Get the ticker symbol and the associated data as per API definition\n",
    "            ticker=ticker_data.get(\"ticker\", \"\")\n",
    "            results=ticker_data.get(\"results\",[])\n",
    "            #Create a record containing the ticker symbol and its associated results.\n",
    "            #Organises data + maintains a consistent format. \n",
    "            record={\n",
    "                \"ticker\":ticker,\n",
    "                \"data\":results,\n",
    "            }\n",
    "\n",
    "            \n",
    "            flattened_result=flat_map(record)\n",
    "            if flattened_result:\n",
    "                accumulated_result.extend(flattened_result)\n",
    "\n",
    "            #write to disk if list exceeds threshold, this prevent OOM errors. \n",
    "            if len(accumulated_result)>=write_batch_size:\n",
    "                write_to_parquet(accumulated_result)\n",
    "                accumulated_result.clear()\n",
    "\n",
    "        if accumulated_result:\n",
    "            writer.write_to_parquet(accumulated_result)\n",
    "\n",
    "        print(\"Processing of a batch complete\")\n",
    "        \n",
    "        print(\"Waiting 60.1secs\")\n",
    "        time.sleep(60.1) #Sleep for 1 minute\n",
    "\n",
    "    return\n",
    "\n",
    "class DynamicWriter():\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path=file_path\n",
    "        self.is_first_write=True\n",
    "\n",
    "    \n",
    "    def write_to_parquet(self, flattened_result):\n",
    "        #Define schema of the dataframe.\n",
    "        schema=StructType(\n",
    "            [\n",
    "                StructField(\"ticker\",StringType(),True),\n",
    "                StructField(\"c\",DoubleType(),True),\n",
    "                StructField(\"t\",LongType(),True)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "        write_mode=\"overwrite\" if self.is_first_write else \"append\"\n",
    "        spark_df = spark.createDataFrame(flattened_result,schema)\n",
    "        spark_df.write.mode(write_mode).parquet(self.file_path)\n",
    "        self.is_first_write=False\n",
    "        print(f\"Data written with {write_mode}\")\n",
    "\n",
    "file_path=\"output_file.parquet\"\n",
    "writer=DynamicWriter(file_path)\n",
    "process_all_tickers_and_write(symbols_to_analyse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "572e8976-bcfb-44e7-8782-ff816f9a4c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|ticker|\n",
      "+------+\n",
      "|    SO|\n",
      "|   CME|\n",
      "|   ADI|\n",
      "|  GILD|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_data(input_file=\"output_file.parquet\"):\n",
    "    df=spark.read.parquet(input_file)\n",
    "    date_col = date_format(from_unixtime(col(\"t\")/1000),\"yyyy-MM-dd\")\n",
    "\n",
    "    df = df.withColumn(\"date\",date_col)\n",
    "    df=df.drop(col(\"t\"))\n",
    "    df = df.withColumnRenamed(\"c\",\"close_price\")\n",
    "    return df\n",
    "df = read_data()\n",
    "df.select(col(\"ticker\")).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8192f44-7755-40f8-afbc-700bddb4bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_price_difference( df, ticker_col='ticker', date_col='date',price_col='close_price'):\n",
    "    #1) Repartition by ticker means grouping/aggregations can be done locally (per ticker). This reduces shuffling and maximises parrallelism.\n",
    "    #data frame is partitioned and sorted for group operations\n",
    "    #2) Order by is needed due to make fetching the first and last prices are correct and efficient. \n",
    "    #3) Round, filtering, percent calcs completed in lower number of transformtions. Increases readability.\n",
    "    #4) Wrapping function allows reusuability, modularity, paramaterisation (for different datasets in other code bases), can write unit tests for small datasets\n",
    "    #and see if it has expected results.\n",
    "\n",
    "    df = df.repartition(ticker_col).orderBy(ticker_col,date_col)\n",
    "\n",
    "    window_spec = Window.partitionBy(\"ticker\").orderBy(date_col).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    \n",
    "    df_diff=df.withColumn(\"price_first\", F.first(price_col).over(window_spec)) \\\n",
    "                .withColumn(\"price_last\", F.last(price_col).over(window_spec)) \\\n",
    "                .withColumn(\"date_first\",F.first(date_col).over(window_spec)) \\\n",
    "                .withColumn(\"date_last\",F.last(date_col).over(window_spec)) \\\n",
    "                .filter((F.col(date_col) == F.col(\"date_first\")) | (F.col(date_col) == F.col(\"date_last\")))\n",
    "    \n",
    "    \n",
    "    df_diff=df_diff.withColumn(\"price_diff\", F.round(F.col(\"price_last\")-F.col(\"price_first\"),2)) \\\n",
    "                .withColumn(\"price_diff_percent\", F.round(col(\"price_diff\")*100/col(\"price_first\"),2))\n",
    "    \n",
    "    df_diff_percent=df_diff.select(\"ticker\", \"date_first\",\"date_first\", \"price_first\", \"date_last\",\"price_last\",\"price_diff\",\"price_diff_percent\"). \\\n",
    "                    distinct(). \\\n",
    "                    orderBy(F.col(\"price_diff_percent\").desc())\n",
    "    \n",
    "    return df_diff_percent\n",
    "\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4e894-4b19-4612-a31c-20e0cdf79951",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#QUESTION 1 ANSWER: Which stock has had the greatest relative increase in price in this period? 'NVDA'\n",
    "###############################\n",
    "\n",
    "def greatest_relative_increase(df_diff_percent):\n",
    "    #Get the first row and stock name.\n",
    "    stock_name = df_diff_percent.collect()[0][\"ticker\"]\n",
    "    stock_name =df_stock_name.filter(col(\"symbol\")==stock_name).show(truncate=False)\n",
    "    return stock_name\n",
    "    \n",
    "df_diff_percent= calculate_price_difference(df)\n",
    "df_diff_percent.show(truncate=False)\n",
    "greatest_relative_increase(df_diff_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564895fa-f19c-4be3-92d5-3b556b690e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "26454937-a328-481c-8416-7d036f1cf7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9b0a860c-72b0-4c80-b452-3f5a76425a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find how much this percentage increase would increase your intial portfolio\n",
    "#At the end of the year.\n",
    "def final_portfolio_value(df_diff_percent):\n",
    "    #Find the average percentage increase over the group of stocks in 2023.\n",
    "    df_diff_percent_average=df_diff_percent.agg(F.avg(\"price_diff_percent\").alias(\"avg_percent_gain\"))\n",
    "    df_diff_percent_average=df_diff_percent_average.collect()[0][\"avg_percent_gain\"]\n",
    "    \n",
    "    initial_portfolio_value=1000000\n",
    "    final_portfolio_value=initial_portfolio_value*(1+df_diff_percent_average/100)\n",
    "    final_portfolio_value = round(final_portfolio_value,2)\n",
    "    return final_portfolio_value\n",
    "#QUESTION 2 ANSWER: If you had invested $1 million at the beginning of this period by purchasing $10,000 worth of shares in every company in the list equally, \n",
    "#how much would you have today? Technical note, you can assume that it is possible \n",
    "#to purchase fractional shares. Ans: $1682181.14\n",
    "final_portfolio_value(df_diff_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ca7d3fee-b655-4a71-9d56-9c6e4261225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e27236c1-12e1-4c32-8d32-9e5f2fb3aed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|company_name|symbol|\n",
      "+------------+------+\n",
      "|Apple Inc.  |AAPL  |\n",
      "+------------+------+\n",
      "\n",
      "Apple Inc.\n"
     ]
    }
   ],
   "source": [
    "def monthly_cagr(df, start_month, end_month):\n",
    "\n",
    "    #Filter dataframe for data on these dates. Make 2 seperate dataframes for each of these dates. \n",
    "    df_start=df.filter(col(\"date\")==start_month)\n",
    "    df_start=df_start.withColumnRenamed(\"close_price\",\"start_price\")\n",
    "    df_start=df_start.withColumnRenamed(\"date\",\"start_month\")\n",
    "    if df_start.isEmpty():\n",
    "        raise ValueError(\"df_start dataset is empty\")\n",
    "\n",
    "    #Shows the end date and end prince we are interested in. \n",
    "    df_end=df.filter(col(\"date\")==end_month)\n",
    "    df_end=df_end.withColumnRenamed(\"close_price\",\"end_price\")\n",
    "    df_end=df_end.withColumnRenamed(\"date\",\"end_month\")\n",
    "    if df_end.isEmpty():\n",
    "        raise ValueError(\"df_end dataset is empty\")\n",
    "\n",
    "    #Join these dataframes on primary key ticker. \n",
    "    df_jan_jun=df_start.join(df_end,on=\"ticker\")\n",
    "\n",
    "    #For extensibilty for the month constant in below calc\n",
    "    from datetime import datetime\n",
    "    start_date=datetime.strptime(start_month,\"%Y-%m-%d\")\n",
    "    end_date=datetime.strptime(end_month,\"%Y-%m-%d\")\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    difference=relativedelta(end_date,start_date)\n",
    "    difference_months=difference.years*12+difference.months\n",
    "\n",
    "    #calculate the monthly Compounded annual growth rate.\n",
    "    #Assumption of inclusive of January and June months\n",
    "    end_val = col(\"end_price\")\n",
    "    start_val = col(\"start_price\")\n",
    "    months = difference_months+1\n",
    "    \n",
    "    calculation = ((end_val/start_val)**(1/months))-1\n",
    "    df_jan_jun=df_jan_jun.withColumn(\"CAGR over defined period\", calculation)\n",
    "\n",
    "    #Sort datafram in \"percent_gain\" decending order\n",
    "    df_jan_jun=df_jan_jun.sort(col(\"CAGR over defined period\"), ascending=False)\n",
    "\n",
    "    #Pick the top result\n",
    "    stock_with_greatest_monthly_CAGR=df_jan_jun.collect()[0][\"ticker\"]\n",
    "    stock_with_greatest_monthly_CAGR=df_stock_name.filter(col(\"symbol\")==stock_with_greatest_monthly_CAGR)\n",
    "    stock_with_greatest_monthly_CAGR.show(truncate=False)\n",
    "    return print(stock_with_greatest_monthly_CAGR.collect()[0][\"company_name\"])\n",
    "    \n",
    "df = read_data()\n",
    "stock_with_greatest_monthly_CAGR=monthly_cagr(df, start_month=\"2023-01-03\", end_month=\"2023-06-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961244a8-2dbc-4f2a-b901-7654e73d5c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e7a91cb-285d-4f44-b9d9-1eb00183d983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248eb9e-b833-4ca9-8428-f57e6b2f4e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e7482d3d-cf89-42c6-996a-64106dc78699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-----------+---------+----------+\n",
      "|ticker|         week_start|start_price|end_price|price_diff|\n",
      "+------+-------------------+-----------+---------+----------+\n",
      "|  MSFT|2023-01-02 00:00:00|     239.58|   224.93|    -14.65|\n",
      "|  AAPL|2023-07-31 00:00:00|     196.45|   181.99|    -14.46|\n",
      "|  MSFT|2023-02-13 00:00:00|     271.32|   258.06|    -13.26|\n",
      "|  MSFT|2023-09-18 00:00:00|     329.06|   317.01|    -12.05|\n",
      "|  AAPL|2023-09-04 00:00:00|      189.7|   178.18|    -11.52|\n",
      "|  MSFT|2023-06-05 00:00:00|     335.94|   326.79|     -9.15|\n",
      "|  MSFT|2023-08-07 00:00:00|     330.11|   321.01|      -9.1|\n",
      "|  MSFT|2023-03-06 00:00:00|     256.87|   248.59|     -8.28|\n",
      "|  MSFT|2023-07-31 00:00:00|     335.92|   327.78|     -8.14|\n",
      "|  MSFT|2023-09-11 00:00:00|     337.94|   330.22|     -7.72|\n",
      "|  MSFT|2023-08-14 00:00:00|     324.04|   316.48|     -7.56|\n",
      "|  MSFT|2023-07-24 00:00:00|     345.11|   338.37|     -6.74|\n",
      "|  MSFT|2023-10-16 00:00:00|     332.64|   326.67|     -5.97|\n",
      "|  AAPL|2023-10-16 00:00:00|     178.72|   172.88|     -5.84|\n",
      "|  AAPL|2023-03-06 00:00:00|     153.83|    148.5|     -5.33|\n",
      "|  AAPL|2023-08-14 00:00:00|     179.46|   174.49|     -4.97|\n",
      "|  AAPL|2023-09-25 00:00:00|     176.08|   171.21|     -4.87|\n",
      "|  AAPL|2023-10-23 00:00:00|      173.0|   168.22|     -4.78|\n",
      "|  AAPL|2023-09-11 00:00:00|     179.36|   175.01|     -4.35|\n",
      "|  MSFT|2023-11-27 00:00:00|     378.61|   374.51|      -4.1|\n",
      "+------+-------------------+-----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "MSFT\n",
      "2023-01-02 00:00:00\n",
      "+--------------------+------+\n",
      "|        company_name|symbol|\n",
      "+--------------------+------+\n",
      "|Microsoft Corpora...|  MSFT|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Microsoft Corporation'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Start Question 4\n",
    "def greatest_decrease_in_price(df):\n",
    "\n",
    "    df_week = df.withColumn(\"week_start\",F.date_trunc(\"week\",col(\"date\")))\n",
    "    #Window partition by ticker and week\n",
    "    window_spec=Window.partitionBy(\"ticker\", \"week_start\").orderBy(\"week_start\")\n",
    "    df_week=df_week.withColumn(\"start_price\",F.first(\"close_price\").over(window_spec)) \\\n",
    "                .withColumn(\"end_price\",F.last(\"close_price\").over(window_spec))\n",
    "    df_week_final=df_week.select(\"ticker\",\"week_start\",\"start_price\",\"end_price\").distinct()\n",
    "    df_week_final=df_week_final.withColumn(\"price_diff\",F.round(F.col(\"end_price\")-F.col(\"start_price\"),2))\n",
    "    df_week_final = df_week_final.sort(col(\"price_diff\"),ascending=True)\n",
    "    df_week_final.show()\n",
    "    greatest_price_drop_stock = df_week_final.collect()[0][\"ticker\"]\n",
    "    greatest_price_drop_week = df_week_final.collect()[0][\"week_start\"]\n",
    "    print(greatest_price_drop_stock)\n",
    "    print(greatest_price_drop_week)\n",
    "    greatest_price_drop_stock_name= df_stock_name.filter(col(\"symbol\")==greatest_price_drop_stock)\n",
    "    greatest_price_drop_stock_name.show()\n",
    "    return greatest_price_drop_stock_name.collect()[0][\"company_name\"]\n",
    "greatest_decrease_in_price(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a68e0b42-1918-416a-b247-9e3334d7a61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-31 00:00:00\n",
      "Apple Inc.\n"
     ]
    }
   ],
   "source": [
    "def greatest_percent_drop_stock(df):\n",
    "    df_week = df.withColumn(\"week_start\",F.date_trunc(\"week\",col(\"date\")))\n",
    "    #Window partition by ticker and week\n",
    "    window_spec=Window.partitionBy(\"ticker\", \"week_start\").orderBy(\"week_start\")\n",
    "    df_week=df_week.withColumn(\"start_price\",F.first(\"close_price\").over(window_spec)) \\\n",
    "                  .withColumn(\"end_price\",F.last(\"close_price\").over(window_spec))\n",
    "    df_week_final=df_week.select(\"ticker\",\"week_start\",\"start_price\",\"end_price\").distinct()\n",
    "    df_week_final=df_week_final.withColumn(\"price_diff\",F.round(F.col(\"end_price\")-F.col(\"start_price\"),2))\n",
    "    calc= (col(\"price_diff\"))*100/col(\"start_price\")\n",
    "    df_week_final_percent=df_week_final.withColumn(\"percent_drop\",calc). \\\n",
    "                            sort(col(\"percent_drop\"),ascending=True)\n",
    "    \n",
    "    \n",
    "    greatest_percent_drop_stock = df_week_final_percent.collect()[0][\"ticker\"]    \n",
    "    greatest_percent_drop_week =df_week_final_percent.collect()[0][\"week_start\"]\n",
    "    \n",
    "    print(greatest_percent_drop_week)\n",
    "    \n",
    "    greatest_percent_drop_stock_full_name = df_stock_name.filter(col(\"symbol\")==greatest_percent_drop_stock)\n",
    "    greatest_percent_drop_stock_full_name = greatest_percent_drop_stock_full_name.collect()[0][\"company_name\"]\n",
    "    print(greatest_percent_drop_stock_full_name)\n",
    "    return\n",
    "    \n",
    "greatest_percent_drop_stock(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd4bfd-083a-4a56-9853-2777c2fbea7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2b6f8-3a33-4a26-bef8-0afefa94bdf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb7d22-2254-404f-85b5-cac3ee21c75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
